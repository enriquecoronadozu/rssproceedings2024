<html> 
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />  
<head>  
<meta name="citation_title" content="Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning" />  
<meta name="citation_author" content="Vitalis Vosylius, Younggyo Seo, Jafar Uruç, Stephen James" />  
<meta name="citation_publication_date" content="2024/07/15" />  
<meta name="citation_conference_title" content="Robotics: Science and Systems XX" />  
<meta name="citation_isbn" content="979-8-9902848-0-7" />  
<meta name="citation_volume" content="20" />  
<meta name="citation_pdf_url" content="http://www.roboticsproceedings.org/rss20/p051.pdf" />  
<title>Robotics: Science and Systems XX - Online Proceedings</title>  
<link type="text/css" rel="stylesheet" href="../style.css">  
<!--[if IE 5]> 
<link type="text/css" rel="stylesheet" href="../ie5.css"> 
<![endif]-->  
<!--[if IE 6]> 
<link type="text/css" rel="stylesheet" href="../ie6.css"> 
<![endif]-->  
</head>  
<body>  
<div class="menu">  
<%include="menu-fragment.html"%> 
</div>  
<div class="content">  
  
<h1>Robotics: Science and Systems XX</h1>  
<h3>Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning</h3> 
<i>Vitalis Vosylius, Younggyo Seo, Jafar Uruç, Stephen James</i><br> 
<p>  
<b>Abstract:</b>  
</p><p style="text-align: justify;">  
In the field of Robot Learning, the complex mapping between high-dimensional observations such as RGB images and low-level robotic actions, two inherently very different spaces, constitutes a complex learning problem, especially with limited amounts of data. In this work, we introduce Render and Diffuse (R&D) a method that unifies low-level robot actions and RGB observations within the image space using virtual renders of the 3D model of the robot. Using this joint observation-action representation it computes low-level robot actions using a learnt diffusion process that iteratively updates the virtual renders of the robot. This space unification simplifies the learning problem and introduces inductive biases that are crucial for sample efficiency and spatial generalisation. We thoroughly evaluate several variants of R&D in simulation and showcase their applicability on six everyday tasks in the real world. Our results show that R&D exhibits strong spatial generalisation capabilities and is more sample efficient than more common image-to-action methods.
</p>  
<p>  
<b>Download:</b> <a href="p051.pdf" target="_blank" onclick="_gaq.push(['_trackEvent','rss10/p051.pdf','PDF',this.href]);"><img src="../icon-pdf.png" border=0></a> 
</p>  
<p>  
<b>Bibtex:</b>  
<pre>  
@INPROCEEDINGS{Vosylius-RSS-24, 
    AUTHOR    = {Vitalis Vosylius, Younggyo Seo, Jafar Uruç, Stephen James}, 
    TITLE     = {{Render and Diffuse: Aligning Image and Action Spaces for Diffusion-based Behaviour Cloning}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2024}, 
    ADDRESS   = {Delft, Netherlands}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2024.XX.051} 
} 
  
</pre>  
</p>  
  
</div>  
<div class="analyt"> 
<%include="analyt-fragment.html"%> 
</div> 
</body>  
</html> 
