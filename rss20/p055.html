<html> 
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />  
<head>  
<meta name="citation_title" content="RVT-2: Learning Precise Manipulation from Few Demonstrations" />  
<meta name="citation_author" content="Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox" />  
<meta name="citation_publication_date" content="2024/07/15" />  
<meta name="citation_conference_title" content="Robotics: Science and Systems XX" />  
<meta name="citation_isbn" content="979-8-9902848-0-7" />  
<meta name="citation_volume" content="20" />  
<meta name="citation_pdf_url" content="http://www.roboticsproceedings.org/rss20/p055.pdf" />  
<title>Robotics: Science and Systems XX - Online Proceedings</title>  
<link type="text/css" rel="stylesheet" href="../style.css">  
<!--[if IE 5]> 
<link type="text/css" rel="stylesheet" href="../ie5.css"> 
<![endif]-->  
<!--[if IE 6]> 
<link type="text/css" rel="stylesheet" href="../ie6.css"> 
<![endif]-->  
</head>  
<body>  
<div class="menu">  
<%include="menu-fragment.html"%> 
</div>  
<div class="content">  
  
<h1>Robotics: Science and Systems XX</h1>  
<h3>RVT-2: Learning Precise Manipulation from Few Demonstrations</h3> 
<i>Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox</i><br> 
<p>  
<b>Abstract:</b>  
</p><p style="text-align: justify;">  
In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.
</p>  
<p>  
<b>Download:</b> <a href="p055.pdf" target="_blank" onclick="_gaq.push(['_trackEvent','rss10/p055.pdf','PDF',this.href]);"><img src="../icon-pdf.png" border=0></a> 
</p>  
<p>  
<b>Bibtex:</b>  
<pre>  
@INPROCEEDINGS{Goyal-RSS-24, 
    AUTHOR    = {Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox}, 
    TITLE     = {{RVT-2: Learning Precise Manipulation from Few Demonstrations}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2024}, 
    ADDRESS   = {Delft, Netherlands}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2024.XX.055} 
} 
  
</pre>  
</p>  
  
</div>  
<div class="analyt"> 
<%include="analyt-fragment.html"%> 
</div> 
</body>  
</html> 
